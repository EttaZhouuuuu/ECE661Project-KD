{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfVx0QHw0YG7",
        "outputId": "418ba262-47b0-44a1-fcc9-18ca54760879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/661project\"\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fr6jGYS0qDH",
        "outputId": "7bc08ac4-026c-4ce5-b9e6-282b9e12847b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/661project\n",
            "/content/drive/MyDrive/661project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYPG48C4z-MZ",
        "outputId": "bf8fa3d5-7193-4ab5-a687-ad7765a8f111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124 0.21.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "print(torch.__version__, torchvision.__version__)\n",
        "from torchvision import datasets   # should now succeed\n",
        "full_train = datasets.CIFAR10(root='data/', train=True, download=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose K = 5 and 10 classes => 2 classes per teacher\n",
        "K = 5\n",
        "classes_per_teacher = 10 // K  # = 2\n",
        "teacher_splits = []\n",
        "targets = full_train.targets  # list of integer labels\n",
        "\n",
        "for t in range(K):\n",
        "    cls_start = t * classes_per_teacher\n",
        "    cls_end   = cls_start + classes_per_teacher\n",
        "    # find indices whose label ∈ [cls_start, cls_end)\n",
        "    idxs = [i for i, lab in enumerate(targets)\n",
        "            if cls_start <= lab < cls_end]\n",
        "    teacher_splits.append(idxs)\n"
      ],
      "metadata": {
        "id": "d-rwESRX1S6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "num_samples = len(full_train)\n",
        "perm = torch.randperm(num_samples).tolist()\n",
        "teacher_splits = []\n",
        "split_size = num_samples // K\n",
        "\n",
        "for t in range(K):\n",
        "    start = t * split_size\n",
        "    end   = start + split_size if t < K-1 else num_samples\n",
        "    teacher_splits.append(perm[start:end])\n"
      ],
      "metadata": {
        "id": "t6PqVttJ1XG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "teacher_datasets = [\n",
        "    Subset(full_train, idxs)\n",
        "    for idxs in teacher_splits\n",
        "]\n",
        "# Now teacher_datasets[i] is the CIFAR-10 subset for teacher #i\n"
      ],
      "metadata": {
        "id": "YnD1SMvF1Zov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# CIFAR-10 normalization constants\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n"
      ],
      "metadata": {
        "id": "FTcaRtgb1Zgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to override the transform of the underlying dataset\n",
        "# for each Subset. One simple approach is to wrap with a lambda:\n",
        "class TransformSubset(Subset):\n",
        "    def __init__(self, subset, transform):\n",
        "        super().__init__(subset.dataset, subset.indices)\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = super().__getitem__(idx)\n",
        "        return self.transform(img), label\n",
        "\n",
        "teacher_load_datasets = [\n",
        "    TransformSubset(ds, train_transform)\n",
        "    for ds in teacher_datasets\n",
        "]\n"
      ],
      "metadata": {
        "id": "ZQlBHk2g1ZVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size  = 128\n",
        "num_workers = 4\n",
        "\n",
        "teacher_loaders = [\n",
        "    DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    for ds in teacher_load_datasets\n",
        "]\n",
        "\n",
        "# And for your student’s full training set (hard‑label baseline):\n",
        "full_train.transform = train_transform\n",
        "student_train_loader = DataLoader(\n",
        "    full_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# CIFAR-10 test set:\n",
        "test_set = datasets.CIFAR10(\n",
        "    root='data/',\n",
        "    train=False,\n",
        "    download=False,\n",
        "    transform=test_transform\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Lfzh92ki1ZK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OrEsYUI1-PJ",
        "outputId": "b8e3b17d-ea5a-47ad-8014-30c4c2c6c0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18, resnet34, resnet50\n",
        "from torchinfo import summary\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2.1 Instantiate K teacher models (here: ResNet18, ResNet34, ResNet50)\n",
        "teacher_archs = ['resnet18', 'resnet34', 'resnet50']\n",
        "teachers = []\n",
        "\n",
        "for arch in teacher_archs:\n",
        "    if arch == 'resnet18':\n",
        "        model = resnet18(num_classes=10)\n",
        "    elif arch == 'resnet34':\n",
        "        model = resnet34(num_classes=10)\n",
        "    elif arch == 'resnet50':\n",
        "        model = resnet50(num_classes=10)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "    model.to(device)\n",
        "    teachers.append((arch, model))\n",
        "\n",
        "# 2.2 Instantiate the student model (smaller-capacity): ResNet18\n",
        "student = resnet18(num_classes=10)\n",
        "student.to(device)\n",
        "\n",
        "# 2.3 Parameter & FLOPs statistics using torchinfo.summary\n",
        "# Assume CIFAR-10 input size: (batch_size=1, channels=3, height=32, width=32)\n",
        "input_size = (1, 3, 32, 32)\n",
        "\n",
        "print(\"\\n=== Teacher Models ===\")\n",
        "for name, model in teachers:\n",
        "    print(f\"\\n-- {name.upper()} --\")\n",
        "    summary(model, input_size=input_size, col_names=(\"output_size\", \"num_params\", \"mult_adds\"))\n",
        "\n",
        "print(\"\\n=== Student Model ===\")\n",
        "print(\"-- RESNET18 STUDENT --\")\n",
        "summary(student, input_size=input_size, col_names=(\"output_size\", \"num_params\", \"mult_adds\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYMZN8sw14Cm",
        "outputId": "6a0385de-4cf8-4d50-a61a-addbcf4a5141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Teacher Models ===\n",
            "\n",
            "-- RESNET18 --\n",
            "\n",
            "-- RESNET34 --\n",
            "\n",
            "-- RESNET50 --\n",
            "\n",
            "=== Student Model ===\n",
            "-- RESNET18 STUDENT --\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #                   Mult-Adds\n",
              "===================================================================================================================\n",
              "ResNet                                   [1, 10]                   --                        --\n",
              "├─Conv2d: 1-1                            [1, 64, 16, 16]           9,408                     2,408,448\n",
              "├─BatchNorm2d: 1-2                       [1, 64, 16, 16]           128                       128\n",
              "├─ReLU: 1-3                              [1, 64, 16, 16]           --                        --\n",
              "├─MaxPool2d: 1-4                         [1, 64, 8, 8]             --                        --\n",
              "├─Sequential: 1-5                        [1, 64, 8, 8]             --                        --\n",
              "│    └─BasicBlock: 2-1                   [1, 64, 8, 8]             --                        --\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 8, 8]             36,864                    2,359,296\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 8, 8]             128                       128\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 8, 8]             --                        --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 8, 8]             36,864                    2,359,296\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 8, 8]             128                       128\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 8, 8]             --                        --\n",
              "│    └─BasicBlock: 2-2                   [1, 64, 8, 8]             --                        --\n",
              "│    │    └─Conv2d: 3-7                  [1, 64, 8, 8]             36,864                    2,359,296\n",
              "│    │    └─BatchNorm2d: 3-8             [1, 64, 8, 8]             128                       128\n",
              "│    │    └─ReLU: 3-9                    [1, 64, 8, 8]             --                        --\n",
              "│    │    └─Conv2d: 3-10                 [1, 64, 8, 8]             36,864                    2,359,296\n",
              "│    │    └─BatchNorm2d: 3-11            [1, 64, 8, 8]             128                       128\n",
              "│    │    └─ReLU: 3-12                   [1, 64, 8, 8]             --                        --\n",
              "├─Sequential: 1-6                        [1, 128, 4, 4]            --                        --\n",
              "│    └─BasicBlock: 2-3                   [1, 128, 4, 4]            --                        --\n",
              "│    │    └─Conv2d: 3-13                 [1, 128, 4, 4]            73,728                    1,179,648\n",
              "│    │    └─BatchNorm2d: 3-14            [1, 128, 4, 4]            256                       256\n",
              "│    │    └─ReLU: 3-15                   [1, 128, 4, 4]            --                        --\n",
              "│    │    └─Conv2d: 3-16                 [1, 128, 4, 4]            147,456                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-17            [1, 128, 4, 4]            256                       256\n",
              "│    │    └─Sequential: 3-18             [1, 128, 4, 4]            8,448                     131,328\n",
              "│    │    └─ReLU: 3-19                   [1, 128, 4, 4]            --                        --\n",
              "│    └─BasicBlock: 2-4                   [1, 128, 4, 4]            --                        --\n",
              "│    │    └─Conv2d: 3-20                 [1, 128, 4, 4]            147,456                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-21            [1, 128, 4, 4]            256                       256\n",
              "│    │    └─ReLU: 3-22                   [1, 128, 4, 4]            --                        --\n",
              "│    │    └─Conv2d: 3-23                 [1, 128, 4, 4]            147,456                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-24            [1, 128, 4, 4]            256                       256\n",
              "│    │    └─ReLU: 3-25                   [1, 128, 4, 4]            --                        --\n",
              "├─Sequential: 1-7                        [1, 256, 2, 2]            --                        --\n",
              "│    └─BasicBlock: 2-5                   [1, 256, 2, 2]            --                        --\n",
              "│    │    └─Conv2d: 3-26                 [1, 256, 2, 2]            294,912                   1,179,648\n",
              "│    │    └─BatchNorm2d: 3-27            [1, 256, 2, 2]            512                       512\n",
              "│    │    └─ReLU: 3-28                   [1, 256, 2, 2]            --                        --\n",
              "│    │    └─Conv2d: 3-29                 [1, 256, 2, 2]            589,824                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-30            [1, 256, 2, 2]            512                       512\n",
              "│    │    └─Sequential: 3-31             [1, 256, 2, 2]            33,280                    131,584\n",
              "│    │    └─ReLU: 3-32                   [1, 256, 2, 2]            --                        --\n",
              "│    └─BasicBlock: 2-6                   [1, 256, 2, 2]            --                        --\n",
              "│    │    └─Conv2d: 3-33                 [1, 256, 2, 2]            589,824                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-34            [1, 256, 2, 2]            512                       512\n",
              "│    │    └─ReLU: 3-35                   [1, 256, 2, 2]            --                        --\n",
              "│    │    └─Conv2d: 3-36                 [1, 256, 2, 2]            589,824                   2,359,296\n",
              "│    │    └─BatchNorm2d: 3-37            [1, 256, 2, 2]            512                       512\n",
              "│    │    └─ReLU: 3-38                   [1, 256, 2, 2]            --                        --\n",
              "├─Sequential: 1-8                        [1, 512, 1, 1]            --                        --\n",
              "│    └─BasicBlock: 2-7                   [1, 512, 1, 1]            --                        --\n",
              "│    │    └─Conv2d: 3-39                 [1, 512, 1, 1]            1,179,648                 1,179,648\n",
              "│    │    └─BatchNorm2d: 3-40            [1, 512, 1, 1]            1,024                     1,024\n",
              "│    │    └─ReLU: 3-41                   [1, 512, 1, 1]            --                        --\n",
              "│    │    └─Conv2d: 3-42                 [1, 512, 1, 1]            2,359,296                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-43            [1, 512, 1, 1]            1,024                     1,024\n",
              "│    │    └─Sequential: 3-44             [1, 512, 1, 1]            132,096                   132,096\n",
              "│    │    └─ReLU: 3-45                   [1, 512, 1, 1]            --                        --\n",
              "│    └─BasicBlock: 2-8                   [1, 512, 1, 1]            --                        --\n",
              "│    │    └─Conv2d: 3-46                 [1, 512, 1, 1]            2,359,296                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-47            [1, 512, 1, 1]            1,024                     1,024\n",
              "│    │    └─ReLU: 3-48                   [1, 512, 1, 1]            --                        --\n",
              "│    │    └─Conv2d: 3-49                 [1, 512, 1, 1]            2,359,296                 2,359,296\n",
              "│    │    └─BatchNorm2d: 3-50            [1, 512, 1, 1]            1,024                     1,024\n",
              "│    │    └─ReLU: 3-51                   [1, 512, 1, 1]            --                        --\n",
              "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --                        --\n",
              "├─Linear: 1-10                           [1, 10]                   5,130                     5,130\n",
              "===================================================================================================================\n",
              "Total params: 11,181,642\n",
              "Trainable params: 11,181,642\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 37.03\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.81\n",
              "Params size (MB): 44.73\n",
              "Estimated Total Size (MB): 45.55\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a directory for checkpoints\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# 1) Setup device & data\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.247,0.243,0.261)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.247,0.243,0.261)),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10('data', train=True,  download=True, transform=transform_train)\n",
        "test_ds  = datasets.CIFAR10('data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "opt_s = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "sch_s = optim.lr_scheduler.MultiStepLR(opt_s, milestones=[100,150], gamma=0.1)\n",
        "\n",
        "# 2) Helpers\n",
        "def train_epoch(m, loader, opt, crit):\n",
        "    m.train()\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss = crit(m(x), y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "def train_epoch_kd(student, teachers, loader, opt, alpha, T):\n",
        "    student.train()\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            avg_soft = None\n",
        "            for t in teachers:\n",
        "                s = F.softmax(t(x).div(T), dim=1)\n",
        "                avg_soft = s if avg_soft is None else avg_soft + s\n",
        "            avg_soft /= len(teachers)\n",
        "        logits = student(x)\n",
        "        loss_h = F.cross_entropy(logits, y)\n",
        "        loss_s = F.kl_div(\n",
        "            F.log_softmax(logits.div(T), dim=1),\n",
        "            avg_soft,\n",
        "            reduction='batchmean'\n",
        "        ) * (T*T)\n",
        "        (1-alpha)*loss_h + alpha*loss_s\n",
        "        opt.zero_grad()\n",
        "        ( (1-alpha)*loss_h + alpha*loss_s ).backward()\n",
        "        opt.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(m, loader):\n",
        "    m.eval()\n",
        "    correct = total = 0\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        correct += (m(x).argmax(1)==y).sum().item()\n",
        "        total   += y.size(0)\n",
        "    return correct/total\n",
        "\n",
        "# 3) Model factories\n",
        "def get_resnet34():\n",
        "    m = models.resnet34(weights=None, num_classes=10)\n",
        "    m.conv1 = nn.Conv2d(3,64,3,1,1,bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    return m.to(device)\n",
        "\n",
        "def get_resnet50():\n",
        "    m = models.resnet50(weights=None, num_classes=10)\n",
        "    m.conv1 = nn.Conv2d(3,64,3,1,1,bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    return m.to(device)\n",
        "\n",
        "# 4) Train & save student baseline\n",
        "student = get_resnet34()\n",
        "opt_s = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "sch_s = optim.lr_scheduler.MultiStepLR(opt_s, milestones=[100,150], gamma=0.1)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "for _ in range(200):\n",
        "    train_epoch(student, train_loader, opt_s, crit)\n",
        "    sch_s.step()\n",
        "\n",
        "acc_s = evaluate(student, test_loader)\n",
        "print(f\"Student baseline accuracy: {acc_s:.4f}\")\n",
        "torch.save(student.state_dict(), 'checkpoints/student_baseline.pth')\n",
        "\n",
        "# 5) Train & save snapshot-ensemble teachers\n",
        "teacher = get_resnet50()\n",
        "opt_t = optim.SGD(teacher.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "sch_t = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt_t, T_0=40, T_mult=1)\n",
        "\n",
        "snapshots = []\n",
        "for epoch in range(200):\n",
        "    train_epoch(teacher, train_loader, opt_t, crit)\n",
        "    sch_t.step()\n",
        "    if (epoch+1)%40==0:\n",
        "        path = f'checkpoints/teacher_snapshot_{len(snapshots)+1}.pth'\n",
        "        torch.save(teacher.state_dict(), path)\n",
        "        snapshots.append(path)\n",
        "\n",
        "teachers = []\n",
        "for i,p in enumerate(snapshots,1):\n",
        "    m = get_resnet50()\n",
        "    m.load_state_dict(torch.load(p))\n",
        "    m.eval()\n",
        "    teachers.append(m)\n",
        "    acc_t = evaluate(m, test_loader)\n",
        "    print(f\"Teacher #{i} baseline accuracy: {acc_t:.4f}\")\n",
        "\n",
        "# 6) Multi-teacher KD\n",
        "alpha, T = 0.7, 5\n",
        "for K in range(1,6):\n",
        "    kd_stud = get_resnet34()\n",
        "    opt_k = optim.SGD(kd_stud.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    sch_k = optim.lr_scheduler.MultiStepLR(opt_k, [100,150], gamma=0.1)\n",
        "    for _ in range(200):\n",
        "        train_epoch_kd(kd_stud, teachers[:K], train_loader, opt_k, alpha, T)\n",
        "        sch_k.step()\n",
        "    acc_k = evaluate(kd_stud, test_loader)\n",
        "    print(f\"KD with K={K} teachers → accuracy: {acc_k:.4f}\")\n",
        "    torch.save(kd_stud.state_dict(), f'checkpoints/student_k{K}.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXiKAbZ4oXBU",
        "outputId": "4dccf328-8503-4213-c2a8-9b994d1d801a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student baseline accuracy: 0.9542\n",
            "Teacher #1 baseline accuracy: 0.9252\n",
            "Teacher #2 baseline accuracy: 0.9419\n",
            "Teacher #3 baseline accuracy: 0.9448\n",
            "Teacher #4 baseline accuracy: 0.9485\n",
            "Teacher #5 baseline accuracy: 0.9478\n",
            "KD with K=1 teachers → accuracy: 0.9380\n",
            "KD with K=2 teachers → accuracy: 0.9492\n",
            "KD with K=3 teachers → accuracy: 0.9510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beacuse of runtime disconnect了，so we continue on runing the below code for\n",
        "k=4,5\n"
      ],
      "metadata": {
        "id": "vBQJQ5HALE06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup CIFAR-10 data loaders, KD helper, evaluation, and ResNet-34 factory\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1) Data loaders\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.247,0.243,0.261)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.247,0.243,0.261)),\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n",
        "    batch_size=128, shuffle=True, num_workers=4, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.CIFAR10('data', train=False, download=True, transform=transform_test),\n",
        "    batch_size=256, shuffle=False, num_workers=4, pin_memory=True\n",
        ")\n",
        "\n",
        "# 2) Knowledge-distillation training epoch\n",
        "def train_epoch_kd(student, teachers, loader, optimizer, alpha, T):\n",
        "    student.train()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            avg_soft = None\n",
        "            for t in teachers:\n",
        "                t = t.to(device)\n",
        "                p = F.softmax(t(x) / T, dim=1)\n",
        "                avg_soft = p if avg_soft is None else avg_soft + p\n",
        "            avg_soft /= len(teachers)\n",
        "        logits = student(x)\n",
        "        loss_h = F.cross_entropy(logits, y)\n",
        "        loss_s = F.kl_div(\n",
        "            F.log_softmax(logits / T, dim=1),\n",
        "            avg_soft, reduction='batchmean'\n",
        "        ) * (T * T)\n",
        "        loss = (1 - alpha) * loss_h + alpha * loss_s\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# 3) Evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total   += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# 4) ResNet-34 factory\n",
        "def get_resnet34():\n",
        "    m = models.resnet34(weights=None, num_classes=10)\n",
        "    # Adapt first conv for CIFAR-10 (32×32)\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    return m.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "lNhw4aoAKpN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Re-use your train_loader & test_loader, and train_epoch_kd + evaluate + get_resnet34\n",
        "\n",
        "# Load teachers:\n",
        "teachers = []\n",
        "for i in range(1, 6):\n",
        "    # instantiate fresh ResNet-50 and move to GPU\n",
        "    t = models.resnet50(weights=None, num_classes=10).to(device)\n",
        "    t.conv1 = nn.Conv2d(3,64,3,1,1,bias=False).to(device)\n",
        "    t.maxpool = nn.Identity().to(device)\n",
        "    # load snapshot\n",
        "    ckpt = torch.load(f'checkpoints/teacher_snapshot_{i}.pth', map_location=device)\n",
        "    t.load_state_dict(ckpt)\n",
        "    t.eval()\n",
        "    teachers.append(t)\n",
        "\n",
        "# Distill K=4 and K=5\n",
        "alpha, T = 0.7, 5\n",
        "for K in [4,5]:\n",
        "    # fresh student\n",
        "    student = get_resnet34().to(device)\n",
        "    optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
        "    for epoch in range(200):\n",
        "        train_epoch_kd(student, teachers[:K], train_loader, optimizer, alpha, T)\n",
        "        scheduler.step()\n",
        "    acc = evaluate(student, test_loader)\n",
        "    print(f\"KD with K={K} teachers → accuracy: {acc:.4f}\")\n",
        "    torch.save(student.state_dict(), f'checkpoints/student_k{K}.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeDusOHBK4WX",
        "outputId": "6b8327bd-4062-4d3f-8a81-547295138127"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KD with K=4 teachers → accuracy: 0.9529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASTE THIS IN A NEW CELL AFTER SETUP ---\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Re-use your train_loader & test_loader, and train_epoch_kd + evaluate + get_resnet34\n",
        "\n",
        "# Load teachers:\n",
        "teachers = []\n",
        "for i in range(1, 6):\n",
        "    # instantiate fresh ResNet-50 and move to GPU\n",
        "    t = models.resnet50(weights=None, num_classes=10).to(device)\n",
        "    t.conv1 = nn.Conv2d(3,64,3,1,1,bias=False).to(device)\n",
        "    t.maxpool = nn.Identity().to(device)\n",
        "    # load snapshot\n",
        "    ckpt = torch.load(f'checkpoints/teacher_snapshot_{i}.pth', map_location=device)\n",
        "    t.load_state_dict(ckpt)\n",
        "    t.eval()\n",
        "    teachers.append(t)\n",
        "\n",
        "# Distill K=4 and K=5\n",
        "alpha, T = 0.7, 5\n",
        "for K in [5]:\n",
        "    # fresh student\n",
        "    student = get_resnet34().to(device)\n",
        "    optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
        "    for epoch in range(200):\n",
        "        train_epoch_kd(student, teachers[:K], train_loader, optimizer, alpha, T)\n",
        "        scheduler.step()\n",
        "    acc = evaluate(student, test_loader)\n",
        "    print(f\"KD with K={K} teachers → accuracy: {acc:.4f}\")\n",
        "    torch.save(student.state_dict(), f'checkpoints/student_k{K}.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMB6F87yrglC",
        "outputId": "0ecc0dab-016f-4017-ba5d-fcb1f434dc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD with K=5 teachers → accuracy: 0.9544\n"
          ]
        }
      ]
    }
  ]
}